{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('combined_news_all.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "def tokenization(indic_string):\n",
    "    tokens = []\n",
    "    for t in indic_tokenize.trivial_tokenize(indic_string):\n",
    "        tokens.append(t)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocesing(name):\n",
    "    try:\n",
    "        name = name.replace('|', '')\n",
    "        name = name.replace('?', '')\n",
    "        name = name.replace(':', '')\n",
    "        name = name.replace(';', '')\n",
    "        name = name.replace(\"'\", '')\n",
    "        name = name.replace('\"', '')\n",
    "        name = name.replace(',', '')\n",
    "        name = name.replace('.', '')\n",
    "        name = name.replace('(', '')\n",
    "        name = name.replace(')', '')\n",
    "        name = name.replace('\\n', '')\n",
    "        name = name.replace('&', '')\n",
    "        name = name.replace('।', '')\n",
    "    except:\n",
    "        pass\n",
    "    tokens = tokenization(name)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eachNews(df):\n",
    "    tokenData = []\n",
    "    for i in df['News']:\n",
    "        tokenData.append(Preprocesing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in tqdm(range(len(news))):\n",
    "    try:\n",
    "        news[\"News\"][i] = news[\"News\"][i].replace('|', '')\n",
    "        news[\"News\"][i] = news[\"News\"][i].replace('?', '')\n",
    "        news[\"News\"][i] = news[\"News\"][i].replace(':', '')\n",
    "        news[\"News\"][i] = news[\"News\"][i].replace(';', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace(\"'\", '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace('\"', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace(',', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace('.', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace('(', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace(')', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace('\\n', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace('&', '')\n",
    "        news[\"News\"][i]=news[\"News\"][i].replace('।', '')\n",
    "    \n",
    "    except:\n",
    "        count+=1\n",
    "        print(count)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from indicnlp import common\n",
    "# The path to the local git repo for Indic NLP library\n",
    "INDIC_NLP_LIB_HOME=r\"D:\\\\Imp Programs\\\\Python\\\\Lib\\site-packages\\\\indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=r\"D:\\\\Imp Programs\\\\Python\\\\Lib\\site-packages\\\\indic_nlp_resources\"\n",
    "\n",
    "# Add library to Python path\n",
    "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))\n",
    "\n",
    "# Set environment variable for resources folder\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "def tokenization(indic_string):\n",
    "    tokens = []\n",
    "    for t in indic_tokenize.trivial_tokenize(indic_string):\n",
    "        tokens.append(t)\n",
    "    return tokens\n",
    "# news['short_description'] = news['short_description'].apply(lambda x: tokenization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('tokenized_data.txt', 'a', encoding=\"utf-8\")\n",
    "\n",
    "tokenData = []\n",
    "for i in news['News']:\n",
    "    tokenData.append(tokenization(i))\n",
    "# print(tokenData)\n",
    "for i in tokenData:\n",
    "    f.write(f'{i}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from indicnlp.tokenize import indic_tokenize\n",
    "listFinal = []\n",
    "\n",
    "for i in f_list:\n",
    "    value = indic_tokenize.trivial_tokenize(i, lang='hi')\n",
    "    listFinal.append(value)\n",
    "print(listFinal) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=open('D:\\\\6th SEM\\Open Lab\\Dataset\\Hindi-Fake-News-Detection\\Data\\\\final_stopwords.txt', encoding = 'utf-8', errors='ignore')\n",
    "stopwords=[]\n",
    "for x in stop:\n",
    "  x = x.replace('\\n', '')\n",
    "  stopwords.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import unicode\n",
    "\n",
    "def clean_stopwords(indic_string):\n",
    "    str_temp = \"\"\n",
    "    rem = []\n",
    "    for words in indic_string:\n",
    "        if unicode(words) not in stopwords:\n",
    "            str_temp+=words\n",
    "            str_temp+=\" \"\n",
    "            rem.append(str_temp)\n",
    "        str_temp = \"\"\n",
    "    return rem\n",
    "\n",
    "# news['short_description'] = news['short_description'].apply(lambda x: clean_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('cleaned_data.txt', 'a', encoding=\"utf-8\")\n",
    "\n",
    "cleanData = []\n",
    "for i in tokenData:\n",
    "    cleanData.append(clean_stopwords(i))\n",
    "# print(tokenData)\n",
    "for i in cleanData:\n",
    "    f.write(f'{i}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    suffixes = {\n",
    "        1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
    "        2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
    "        3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
    "        4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
    "        5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
    "    } #suffixes tin hindi language that change tense, gender, number but not the meaning\n",
    "\n",
    "    stems = []\n",
    "    for word in data:\n",
    "        for L in range(1,5): \n",
    "            if len(word) > L + 1:\n",
    "                for suffix in suffixes[L]:\n",
    "                    if word.endswith(suffix):\n",
    "                        word = word[:-L] #stripping the suffix from the word\n",
    "        if word:\n",
    "            stems.append(word)\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('stemmed_data.txt', 'a', encoding=\"utf-8\")\n",
    "\n",
    "stemData = []\n",
    "for i in cleanData:\n",
    "    stemData.append(stemming(i))\n",
    "# print(tokenData)\n",
    "for i in cleanData:\n",
    "    f.write(f'{i}\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b97bdc2c87e206e8291e5ab4f714e0f91846bfc9349b2ee1278ba67b5e1fb2e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
